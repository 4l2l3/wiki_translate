h1.

This document will provide information about additional available status commands to give users visibility into the system.

h2. jusage
Show system usage broken down by user:
<pre>
$ jusage
USER       SLOTS USED
---------------------
user1             192
user2              96
user3              72
user4             398
...
</pre>

h2. jwaits
Show longest wait time and for what class they are waiting for users with no slots allocated:
<pre>
$ jwaits
USER          WAIT TIME (min)   WAIT CLASS
----------------------------------------------
user1         81.9              long
user2         275.4             xlong       
user3         30.4              medium
</pre>

h2. jclass
Show job numbers broken down by class (job classes are described here: [[gridEngineQueue|Queue Layout]])::
<pre>
$ jclass
JOB CLASSIFICATION              AVAIL   USED  LIMIT
---------------------------------------------------
devel                             626      0   4624
long                                8   1526   1534
medium                              0   1880   1880
short                             626    220   4624
xlong                             615     21    636
</pre>

h2. jprocs
Show all of the processes spawned by your job and what node they are running on.  Fields documented below:
<pre>
$ jprocs 44922
wh-520-3-13: 30222 brs       0.0  0.0   960  16320  4080 qrsh_starter
wh-520-3-13: 30229 brs       0.0  0.0  2068  26620  6655 orted
wh-520-3-13: 30230 brs       101  0.4 121320 266400 66600 vasp
wh-520-3-13: 30231 brs       101  0.4 120860 266920 66730 vasp
wh-520-3-13: 30232 brs       101  0.4 120792 266308 66577 vasp
...
Host         PID   user      CPU% MEM RSS    VMEM   SIZE  COMMAND
</pre>

h2. sss
Show slot status of hardware pools (hardware pools are described here: [[gridEngineQueue|Queue Layout]]):
<pre>
$ sss
HARDWARE POOL                 AVAIL   USED  TOTAL
-------------------------------------------------
pe_MRI_Sun_X4150_hg              70    890    960
pe_RC_Dell_R410_hg              266   1246   1512
pe_RC_Dell_R620_hg              242    574    816
pe_RC_HP_DL165G7_hg               6    138    144
pe_cms_X7DBR-3_hg                 1    255    256
pe_iponomar_HP_DL165G6_hg        33    423    456
pe_oleynik_Dell_SC1435_hg        12    276    288
pe_svarma_HP_SL365_hg            16    176    192
</pre>

h2. ss
Show slot status of individual nodes in the hardware pools.
<pre>
$ ss

PE = pe_C6100_GPU_hg

           Host         Used  Total
-----------------------------------
     wh-520-3-1            2     12
     wh-520-3-2            2     12
     wh-520-3-3            2     12
     wh-520-3-4            2     12
     wh-520-3-5            2     12
     wh-520-3-6            2     12
     wh-520-3-7            3     12
     wh-520-3-8            3     12
-----------------------------------
                          18     96


PE = pe_MRI_Sun_X4150_hg

           Host         Used  Total
-----------------------------------
   svc-3024-1-1            8      8
  svc-3024-1-10            0      8
  svc-3024-1-11            7      8
...

$ ss pe_C6100_GPU_hg

PE = pe_C6100_GPU_hg

           Host         Used  Total
-----------------------------------
     wh-520-3-1            2     12
     wh-520-3-2            2     12
     wh-520-3-3            2     12
     wh-520-3-4            2     12
     wh-520-3-5            2     12
     wh-520-3-6            2     12
     wh-520-3-7            3     12
     wh-520-3-8            3     12
-----------------------------------
                          18     96
</pre>

h2. qs
Queue status:
<pre>
$ qs
CLUSTER QUEUE                   CQLOAD   USED    RES  AVAIL  TOTAL aoACDS  cdsuE  
--------------------------------------------------------------------------------
default                           0.92   3353      0   1199   4624      0     72 
xlong                             0.92    219      0    293    512      0      0
</pre>
