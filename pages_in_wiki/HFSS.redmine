= [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] =
----
[[PageOutline]]

== Description == 

_From Wikipedia:_ [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] is a commercial finite element method solver for electromagnetic structures from Ansoft Corporation.

== Versions ==

*****_13.0.2*, '''14.0''', '_15.0*

== Authorized Users ==

**********ts doing research in the WAMI research group
****WAMI Faculty

== Platforms ==

*****irce HPC Cluster
*****indows HPC Cluster
****Desktop PC

== Installation on Desktop PC ==
p. Installation media for [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] can be downloaded from our site at https://rc.usf.edu/isos using your USF NetID and password.  After version 14, the install media is distributed as Zip files and should easily open within the Windows file manager.

== Batch Execution on Circe HPC (v15) ==
p(. 1. Ensure that your project file resides on your !UStoreFiles drive, preferably under the *Ansoft* folder.  The example below assumes that my project file exists under 

******_/home/b/brs/Ansoft/!ExampleModel'_ on Circe
******_S:\Ansoft\!ExampleModel'_ for UStoreFiles
******_!\\ustorefiles.usf.edu\homes\Ansoft\!ExampleModel'_ for UStoreFiles
p(. 
p(. These paths are all equivalent and point to the same storage.  (Note: your UStoreFiles drive may be mapped to another drive letter besides 'S')
p(. 2. Using *Remote Desktop_', connect to '_circe.rc.usf.edu* and log in with your USF NetID credentials.
p(. 3. Go to *Applications_'-&gt;'''System Tools'''-&gt;'_Terminal* to start a command-line session.
p(. 4. Load the [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] module:
{{{
[user@host ~]$ module add apps/hfss
}}}
p(. 5. Change to the directory in which your model resides:
{{{
[user@host ~]$ cd Ansoft/ExampleModel
[user@host ExampleModel]$
}}}
p(. 6. Submit the job using the *qsub* command:
{{{
[user@host ~]$ qsub -N MyModel.hfss -l nodes=1,ppn=4,h_rt=01:00:00 $hfss_submit
}}}

p. In the example above, we are submitting a job to analyse the designs in !MyModel.hfss using 4 processors on a single server with a maximum runtime of 1 hour.

=== qsub Execution Options ===
p. Submitting your job to Circe allows for several modes of operation, depending on how your analysis settings are configured.  For jobs using the direct sequential solver as in the first example, we can utilize multiple processors on a single server to facilitate speed-up of the application.  Here's example of submitting such a job:

{{{
[user@host ~]$ qsub -N MyModel.hfss -l nodes=1,ppn=4,h_rt=01:00:00 $hfss_submit
}}}

p. The key option for using the sequential solver is to set *nodes_' equal to 1 and to have '_ppn* be less than or equal to 16.

p. For jobs configured to use Solver Domains, we can use the Distributed analysis type.  Distributed analysis allows us to not only utilize multiple processors on a server, but to utilize multiple servers together in order to reach a solution even more quickly.  Here's an example submission below:

p. The key options for using the distributed (DSO) solver are:

1. Ensure that in your analysis settings, *Enable the use of Solver Domains* is selected
2. Ensure that *nodes* &gt;= 2
3. Ensure that *ppn* &gt;= 16

p. You must specify your job run-time using the *h_rt* option if your job will run for longer than 1 hour.  The time format is in HH:MM:SS.

=== Job Status and Progress Information ===
p. You can view the state of your job in the scheduler by running
{{{
[user@host ~]$ qstat
job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID 
-----------------------------------------------------------------------------------------------------------------
p(. 294322 1.18231 MyModel.hf user         qw    12/18/2012 09:44:13                                   16
}}}

p. This will provide basic information on the job.  The 'state' field will tell you whether the job is running or waiting to be executed.  A state of 'r' indicates the simulation has started.  A state of 'qw' indicates the system is waiting for resources to become available.

p. You can also watch the [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] job log, once the job has started (entered the 'r' state), in order to see the progress of the analysis.  This can be done by 'tailing' the output file.  Using the command below, output that is generated by [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] is automatically printed to your screen as it arrives.  The log file name is &lt;!MyProjectFile&gt;.hfss.out.&lt;!JobIdNumber&gt; where &lt;!MyProjectFile&gt; is the file name submitted for analysis and &lt;!JobIdNumber&gt; is the Job ID number as shown in the first column of the *qstat* output.  This is illustrated below:
{{{
[user@host ExampleModel]$ tail -f MyExample.hfss.out.294322
[progress:  2%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Solving Ports on Local Machine - RUNNING: Building matrix (9:42:03 AM  Dec 18, 2012)
[progress:  8%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Solving Ports on Local Machine - RUNNING: Building matrix (9:42:06 AM  Dec 18, 2012)
[progress: 20%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Solving Ports on Local Machine - RUNNING: Building matrix (9:42:08 AM  Dec 18, 2012)
[progress:  0%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Port-Refining Mesh on Local Machine - RUNNING:   (9:42:10 AM  Dec 18, 2012)
[progress: 15%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Port-Refining Mesh on Local Machine - RUNNING: Reading the mesh stats. (9:42:32 AM  Dec 18, 2012)
[progress: 50%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Port-Refining Mesh on Local Machine - RUNNING: Reading the mesh stats. (9:42:46 AM  Dec 18, 2012)
[progress: 75%] (2) helical_antenna - [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]]Design1 - Setup1: Port-Refining Mesh on Local Machine - RUNNING: Morphing the mesh for curved surfaces (9:42:58 AM  Dec 18, 2012)
}}}

== Batch Execution on Windows HPC (v14) ==
p. Using Windows HPC Server 2008 R2, we can easily dispatch jobs that require more resources than a desktop system can provide.  Configuring your desktop installation to work with the Windows HPC scheduler is a simple process.

=== First-time Steps ===
p. The configuration steps below are required to be performed only once, when setting up [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] to use the Windows HPC cluster:

p(. 1. Install the HPC Pack 2008 R2 Client Utilities.  Directions for doing so are [wiki:HPCPack2008R2Client here].
p(. 
p(. 2. Start the Windows HPC Job manager at *Start_'-&gt;'''All Programs'''-&gt;'''Microsoft HPC Pack 2008 R2'''-&gt;'_HPC Job Manager*.
p(. 
p(. 3. Select "File" and then click on "Select Head Node". You will be prompted to enter the name of a head node.  Enter *rcwhpc-master.forest.usf.edu_' and click '_Ok*.  

p. If you receive a permissions error stating that your account doesn't have access, please email help@usf.edu and ask to be added to the Windows HPC user group on rcwhpc-master.forest.usf.edu.

=== Submitting Jobs ===
p. To submit jobs, you'll need to utilize a job description XML file.  To do this, follow these directions:

p(. 1. After starting the *HPC Job Manager_', go to the '''Actions''' panel on the right side of the HPC Job Manager window and select '_New Job from XML File*
p(. 
p((. !http://rc.usf.edu/images/hfss_xml_1.png!

p(. 2. Select the XML file name by entering *!\\ustorefiles.usf.edu\whpcapps\hfss\&lt;version&gt;\hfss_whpc.xml_', where '_&lt;version&gt;* is either version 13 or 14:
p((((. 
p((. !http://rc.usf.edu/images/hfss_xml_2.png!

p(. 3. You'll want to ensure that your Project file is accessible to the cluster, so we recommend placing it in your UStoreFiles directory or *S:\_' drive (or, Circe home directory).  From there, it can be accessed as a simple UNC, e.g. '_!\\ustorefiles.usf.edu\&lt;username&gt;\Ansoft\!MyProject.hfss*

p(. 4. Set the Job name in the *Job details* section shown below.  This can be whatever name you want to identify the job:
p((((. 
p((. !http://rc.usf.edu/images/hfss_xml_3.png!

p(. 5. Go to  *Edit Tasks_', select the '''[[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] Execute''' task, and click '_Edit...*:

p((. !http://rc.usf.edu/images/hfss_xml_4.png!

p(. 6. In the *Task Details_' window, set your '''Working Directory''' to the directory in which your [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] project file resides (the UNC path mentioned above), and specify the minimum and maximum number of processors you would like your job to use, then click '_Ok*:

p((. !http://rc.usf.edu/images/hfss_xml_5.png!

p(. 7. Select *Environment Variables_' in the '''New Job''' window.  Then click the '''PROJECT_FILE''' entry and '_Edit...*:

p((. !http://rc.usf.edu/images/hfss_xml_6.png!

p(. 8. Set the value in the *Edit Environment Variable_' window to the name of your [[[[[[[[[[[[[[[[[[HFSS]]]]]]]]]]]]]]]]]] project file and click '_Ok*:

p((. !http://rc.usf.edu/images/hfss_xml_7.png!

p(. 9.  Click the *Submit_' button in the '_New Job* window:

p((. !http://rc.usf.edu/images/hfss_xml_8.png!

10.  You should now see your job in the Active jobs list in the main *Job Manager* window:

p((. !http://rc.usf.edu/images/hfss_xml_9.png!

=== Job Status and Progress Information ===
p. Please see our [[WinHPCJobStatus|Windows HPC Job Information page]] for information on managing your jobs, viewing status, and tracking output.

== Getting Help ==
p. Please report any issue to the USF IT Helpdesk by sending an e-mail to help@usf.edu
